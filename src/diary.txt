9-1-2021: prediction su fold 0: LB 0.861 con TTAx4 img_size: 320
9-1-2021: prediction con TTAx9 con nuova pipeline: LB 0.61

9-1-2021: allenamento con pseudo labeling dataset 2019 per 10 epoch su 5 folds
-----
10-1-21:prediction su 5 folds con img_size:320 FATTO LB: 0.877
-----
11-1-21: allenamento e prediction su 5 folds con img_size:512
-> prediction con fold 1: LB 0.871
-> prediction con 5 folds: LB 0.883
-----
12-1-21: provare: freezare il batchnorm layer con:
"for module in model.modules():
    # print(module)
    if isinstance(module, nn.BatchNorm2d):
        if hasattr(module, 'weight'):
            module.weight.requires_grad_(False)
        if hasattr(module, 'bias'):
            module.bias.requires_grad_(False)
        module.eval()"

-> prediction con fold 0: 0.881
-> prediction con 5 folds: LB: 0.889
-----
15-1-21: provare SAM
-> prediction con fold 0: 0.884
-> prediction con fold 0 (GLOUN(NOSAM)+effenetb3): 0.889
-> prediction su 5 folds: 0.890
-----
13-01-21: allenamento di resnext50 con img_size: 512
-> prediction con fold 0 (GLOUN): LB 0.879
-----
DA FARE: oversampling train e prediction
-----
13-01-21: ensembling
-> prediction con fold 0 (Gluon seresnext50+effnetb3): LB 0.884
-----
DA FARE: SAM SU 5 FOLDS effenetb3+resnext50